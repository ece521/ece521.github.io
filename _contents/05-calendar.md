---
id: calendar
name: Calendar
heading: Calendar
subheading: Calendar&#58;
image: ""
---

|           | Optional reading                | Topic
|-----------|------------------------|---------
| **Week 1** |            | 
| Lecture: Monday, Jan 09 |            | Introduction <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec1-intro.pdf)
| Lecture: Thursday, Jan 12 |            | Review of probability <br/> Fundamentals of machine learning <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec2-fundamental.pdf)
| Tutorial |            | Review of linear algebra <br/> Introduction to TensorFlow <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut1.pdf) <br/>  [Tutorial examples](http://www.psi.toronto.edu/~jimmy/ece521/tutorial1.ipynb) as an IPython Notebook. <br/> Take a look at [here](http://jupyter.readthedocs.io/en/latest/install.html) on how to install Jupyter/IPython Notebook.
| **Week 2** |            | 
| Lecture: Monday, Jan 16 | The curse of dimensionality: *Bishop 2006, Chap. 1.4* <br/> K-NN: *Bishop 2006, Chap. 2.5.2* <br/> (free) K-NN and linear regression: *Hastie et al 2013, Chap. 2.3*  <br/> (free) Convex function and Jensen inequality: *MacKay 2003, Chap. 2.7*  <br/> (free) Gradient descent: *Goodfellow et al 2016, Chap. 4.3*      | Example: K Nearest Neighbours <br/> Optimization <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec3-kNNoptim.pdf)
| Lecture: Thursday, Jan 19 |   [Stochastic gradient descent, LÃ©on Bottou](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf) <br/> [The momentum method: *Coursera video: Neural Networks for Machine Learning Lecture 6.3*](https://www.youtube.com/watch?v=LdkkZglLZ0Q) <br/> [Maximum likelihood for a Gaussian: *MacKay 2003, Chap. 22.1*](http://www.inference.phy.cam.ac.uk/itprnn/book.pdf)  <br/> [Maximum likelihhod estimation of a classifier: *Hastie et al 2013, Chap. 2.6.3*](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)   <br/> [Regularization: *Goodfellow et al 2016, Chap. 7.1*](http://www.deeplearningbook.org/contents/regularization.html) <br/> [Regularization through data augmentation: *Goodfellow et al 2016, Chap. 7.4*](http://www.deeplearningbook.org/contents/regularization.html)      | Maximum likelihood estimation (MLE)]<br/> Optimization and regularization <br/>[pdf slides](http://www.mebden.com/ECE521/Lec4.pdf)
| Tutorial |            | Tricks to improve SGD  <br/> "Tuning/debugging" optimizer <br/> Multivariate Gaussian  <br/> Underfitting vs. overfitting  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut2.pdf)
| **Week 3** |            | 
| Lecture: Monday, Jan 23 |            | Probabilistic interpretation of linear regression <br/> MLE vs. MAP <br/> Optimal regressor
| Assignment 1: Wednesday, Jan 25 | Due date: Feb 7 midnight, 2017 <br/> k-NN, Gaussian process (bonus), linear regression            | [Assignment handout](http://www.psi.toronto.edu/~jimmy/ece521/a1.pdf) <br/> Download Tiny MNIST dataset [here](http://www.psi.toronto.edu/~jimmy/ece521/tinymnist.npz)
| Lecture: Thursday, Jan 26 | Regression and decision theory: *Bishop 2006, Chap. 1.5* <br/> Bias-variance trade-off: *Bishop 2006, Chap. 3.2*            | Optimal regressor <br/> Feature expansion <br/> Decision theory <br/> [joint pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec5and6.pdf)
| Tutorial |            | k-NN, Linear regression  <br/> Gaussian process regression <br/> Training, validtion and test set  
